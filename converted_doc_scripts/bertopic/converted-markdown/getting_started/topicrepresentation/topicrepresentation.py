from bertopic import BERTopic
from jet.logger import logger
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer
from transformers import pipeline
from jet.file.utils import save_file
import os
import shutil


OUTPUT_DIR = os.path.join(
    os.path.dirname(__file__), "generated", os.path.splitext(os.path.basename(__file__))[0])
shutil.rmtree(OUTPUT_DIR, ignore_errors=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)
log_file = os.path.join(OUTPUT_DIR, "main.log")
logger.basicConfig(filename=log_file)
logger.info(f"Logs: {log_file}")

"""
The topics that are extracted from BERTopic are represented by words. These words are extracted from the documents
occupying their topics using a class-based TF-IDF. This allows us to extract words that are interesting to a topic but
less so to another.

### **Update Topic Representation after Training**
When you have trained a model and viewed the topics and the words that represent them,
you might not be satisfied with the representation. Perhaps you forgot to remove
stop_words or you want to try out a different n_gram_range. We can use the function `update_topics` to update
the topic representation with new parameters for `c-TF-IDF`:
"""
logger.info("### **Update Topic Representation after Training**")


docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']
docs = docs[:1000]
topic_model = BERTopic(n_gram_range=(2, 3), verbose=True)
topics, probs = topic_model.fit_transform(docs)

"""
From the model created above, one of the most frequent topics is the following:
"""
logger.info("From the model created above, one of the most frequent topics is the following:")

save_file(topic_model.get_topic(31)[:10], f"{OUTPUT_DIR}/frequent_topics.json")
# [('clipper chip', 0.007240771542316232),
#  ('key escrow', 0.004601603973377443),
#  ('law enforcement', 0.004277247929596332),
#  ('intercon com', 0.0035961920238955824),
#  ('amanda walker', 0.003474856425297157),
#  ('serial number', 0.0029876119137150358),
#  ('com amanda', 0.002789303096817983),
#  ('intercon com amanda', 0.0027386688593327084),
#  ('amanda intercon', 0.002585262048515583),
#  ('amanda intercon com', 0.002585262048515583)]

"""
Although there does seems to be some relation between words, it is difficult, at least for me, to intuitively understand
what the topic is about. Instead, let's simplify the topic representation by setting `n_gram_range` to (1, 3) to
also allow for single words.
"""
logger.info("Although there does seems to be some relation between words, it is difficult, at least for me, to intuitively understand")

topic_model.update_topics(docs, n_gram_range=(1, 3))
save_file(topic_model.get_topic(31)[:10], f"{OUTPUT_DIR}/n_gram_topics.json")
# [('encryption', 0.008021846079148017),
#  ('clipper', 0.00789642647602742),
#  ('chip', 0.00637127942464045),
#  ('key', 0.006363124787175884),
#  ('escrow', 0.005030980365244285),
#  ('clipper chip', 0.0048271268437973395),
#  ('keys', 0.0043245812747907545),
#  ('crypto', 0.004311198708675516),
#  ('intercon', 0.0038772934659295076),
#  ('amanda', 0.003516026493904586)]

"""
To me, the combination of the words above seem a bit more intuitive than the words we previously had! You can play
around with `n_gram_range` or use your own custom `sklearn.feature_extraction.text.CountVectorizer` and pass that
instead:
"""
logger.info("To me, the combination of the words above seem a bit more intuitive than the words we previously had! You can play")

vectorizer_model = CountVectorizer(stop_words="english", ngram_range=(1, 5))
topic_model.update_topics(docs, vectorizer_model=vectorizer_model)
save_file(topic_model.get_topic(31)[:10], f"{OUTPUT_DIR}/cleaned_stopwords_topics.json")

"""
!!! Tip "Tip!"
    If you want to change the topics to something else, whether that is merging them or removing outliers, you can pass
    a custom list of topics to update them: `topic_model.update_topics(docs, topics=my_updated_topics)`

### **Custom labels**

The topic labels are currently automatically generated by taking the top 3 words and combining them
using the `_` separator. Although this is an informative label, in practice, this is definitely not the prettiest nor necessarily the most accurate label. For example, although the topic label
`1_space_nasa_orbit` is informative, but we would prefer to have a bit more intuitive label, such as
`space travel`. The difficulty with creating such topic labels is that much of the interpretation is left to the user. Would `space travel` be more accurate or perhaps `space explorations`? To truly understand which labels are most suited, going into some of the documents in topics is especially helpful.

Although we can go through every single topic ourselves and try to label them, we can start by creating an overview of labels that have the length and number of words that we are looking for. To do so, we can generate our list of topic labels with `.generate_topic_labels` and define the number of words, the separator, word length, etc:
"""
logger.info("### **Custom labels**")

topic_labels = topic_model.generate_topic_labels(nr_words=3,
                                                 topic_prefix=False,
                                                 word_length=10,
                                                 separator=", ")

"""
!!! Tip
    If you created [**multiple topic representations**](https://maartengr.github.io/BERTopic/getting_started/multiaspect/multiaspect.html) or aspects, you can choose one of these aspects with `aspect="Aspect1"` or whatever you named the aspect.

In the above example, `1_space_nasa_orbit` would turn into `space, nasa, orbit` since we selected 3 words, no topic prefix, and the `, ` separator. We can then either change our `topic_labels` to whatever we want or directly pass them to `.set_topic_labels` so that they can be used across most visualization functions:
"""
logger.info("If you created [**multiple topic representations**](https://maartengr.github.io/BERTopic/getting_started/multiaspect/multiaspect.html) or aspects, you can choose one of these aspects with `aspect=\"Aspect1\"` or whatever you named the aspect.")

topic_model.set_topic_labels(topic_labels)
save_file(topic_model.get_topic(31)[:10], f"{OUTPUT_DIR}/topic_labels_1.json")

"""
It is also possible to only change a few topic labels at a time by passing a dictionary
where the key represents the *topic ID* and the value is the *topic label*:
"""
logger.info("It is also possible to only change a few topic labels at a time by passing a dictionary")

topic_model.set_topic_labels({1: "Space Travel", 7: "Religion"})
save_file(topic_model.get_topic(31)[:10], f"{OUTPUT_DIR}/topic_labels_2.json")

"""
Then, to make use of those custom topic labels across visualizations, such as `.visualize_hierarchy()`,
we can use the `custom_labels=True` parameter that is found in most visualizations.
"""
logger.info("Then, to make use of those custom topic labels across visualizations, such as `.visualize_hierarchy()`,")

fig = topic_model.visualize_barchart(custom_labels=True)
save_file(fig.to_image, f"{OUTPUT_DIR}/fig.png")

"""
#### Optimize labels
The great advantage of passing custom labels to BERTopic is that when more accurate zero-shot are released,
we can simply use those on top of BERTopic to further fine-tune the labeling. For example, let's say you
have a set of potential topic labels that you want to use instead of the ones generated by BERTopic. You could
use the [bart-large-mnli](https://huggingface.co/facebook/bart-large-mnli) model to find which user-defined
labels best represent the BERTopic-generated labels:
"""
logger.info("#### Optimize labels")

classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")

sequence_to_classify =  " ".join([word for word, _ in topic_model.get_topic(1)])

candidate_labels = ['cooking', 'dancing', 'religion']
save_file(classifier(sequence_to_classify, candidate_labels), f"{OUTPUT_DIR}/classification.json")

logger.info("\n\n[DONE]", bright=True)