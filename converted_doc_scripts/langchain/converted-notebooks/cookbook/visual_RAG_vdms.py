from IPython.display import Video
from huggingface_hub import hf_hub_download
from jet.logger import logger
from langchain.llms.base import LLM
from langchain_community.embeddings.sentence_transformer import (
SentenceTransformerEmbeddings,
)
from langchain_core.callbacks.manager import CallbackManagerForLLMRun
from langchain_core.runnables import ConfigurableField
from langchain_experimental.open_clip import OpenCLIPEmbeddings
from langchain_vdms.vectorstores import VDMS, VDMS_Client
from pathlib import Path
from threading import Thread
from transformers import (
AutoModelForCausalLM,
AutoTokenizer,
TextIteratorStreamer,
set_seed,
)
from typing import Any, List, Mapping, Optional
from zipfile import ZipFile
import cv2
import json
import os
import shutil
import torch
import warnings


OUTPUT_DIR = os.path.join(
    os.path.dirname(__file__), "generated", os.path.splitext(os.path.basename(__file__))[0])
shutil.rmtree(OUTPUT_DIR, ignore_errors=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)
log_file = os.path.join(OUTPUT_DIR, "main.log")
logger.basicConfig(filename=log_file)
logger.info(f"Logs: {log_file}")

PERSIST_DIR = f"{OUTPUT_DIR}/chroma"
os.makedirs(PERSIST_DIR, exist_ok=True)

"""
# Visual RAG using VDMS
Visual RAG is a framework that retrieves video based on provided user prompt. It uses both video scene description generated by open source vision models (ex. video-llama, video-llava etc.) as text embeddings and frames as image embeddings to perform vector similarity search using VDMS.

## Start VDMS Server
Let's start a VDMS docker container using the port 55559.
Keep note of the port and hostname as this is needed for the vector store as it uses the VDMS Python client to connect to the server.
"""
logger.info("# Visual RAG using VDMS")

# ! docker run --rm -d -p 55559:55555 --name vdms_rag_nb intellabs/vdms:latest

"""
## Import Python Packages

Verify the necessary python packages are available for this visual RAG example.
"""
logger.info("## Import Python Packages")

# ! pip install --quiet -U langchain-vdms langchain-experimental sentence-transformers opencv-python open_clip_torch torch accelerate

"""
Now import the packages.
"""
logger.info("Now import the packages.")



set_seed(22)
number_of_frames_per_second = 2

"""
## Initialize Vector Stores
In this section, we initialize the VDMS vector store for both text and images. The text components use model `all-MiniLM-L12-v2`from `SentenceTransformerEmbeddings` and the images use model `ViT-g-14` from `OpenCLIPEmbeddings`.
"""
logger.info("## Initialize Vector Stores")

datapath = Path("./data/visual").resolve()
datapath.mkdir(parents=True, exist_ok=True)

frame_dir = str(datapath / "frames_from_clips")
os.makedirs(frame_dir, exist_ok=True)

vdms_client = VDMS_Client(port=55559)


warnings.filterwarnings("ignore")

text_collection = "text-test"
text_embedder = SentenceTransformerEmbeddings(model_name="all-MiniLM-L12-v2")
text_db = VDMS(
    client=vdms_client,
    embedding=text_embedder,
    collection_name=text_collection,
    engine="FaissFlat",
)

text_retriever = text_db.as_retriever().configurable_fields(
    search_kwargs=ConfigurableField(
        id="k_text_docs",
        name="Search Kwargs",
        description="The search kwargs to use",
    )
)

image_collection = "image-test"
image_embedder = OpenCLIPEmbeddings(
    model_name="ViT-g-14", checkpoint="laion2b_s34b_b88k"
)
image_db = VDMS(
    client=vdms_client,
    embedding=image_embedder,
    collection_name=image_collection,
    engine="FaissFlat",
)
image_retriever = image_db.as_retriever(search_type="mmr").configurable_fields(
    search_kwargs=ConfigurableField(
        id="k_image_docs",
        name="Search Kwargs",
        description="The search kwargs to use",
    )
)

"""
## Data Loading

For this visual RAG example, we need to obtain videos and also video scene descriptions generated by open source vision models (ex. video-llava etc.) as text. 
We have published a [Video Summarization Dataset](https://huggingface.co/datasets/Intel/Video_Summarization_For_Retail) available on Hugging Face which contains short videos of shoppers in a retail setting along with the corresponding textual description of each video.
"""
logger.info("## Data Loading")

hf_hub_download(
    repo_id="Intel/Video_Summarization_For_Retail",
    filename="VideoSumForRetailData.zip",
    repo_type="dataset",
    local_dir=str(datapath),
)
with ZipFile(str(datapath / "VideoSumForRetailData.zip"), "r") as z:
    z.extractall(path=datapath)

with open(str(datapath / "VideoSumForRetailData/clips_anno.json"), "r") as f:
    scene_info = json.load(f)

video_dir = str(datapath / "VideoSumForRetailData/clips/")

video_list = {}
for scene in scene_info:
    video_list[scene["video"].split("/")[-1]] = scene["conversations"][1]["value"]

"""
Here we use OpenCV to extract metadata such as fps and number of frames for each video and also metadata such as frame number and timestamp for each extracted video frame. Once the metadata is extracted, the details are stored in VDMS.
"""
logger.info("Here we use OpenCV to extract metadata such as fps and number of frames for each video and also metadata such as frame number and timestamp for each extracted video frame. Once the metadata is extracted, the details are stored in VDMS.")

text_content = []
video_metadata_list = []
uris = []
frame_metadata_list = []
for video_name, description in video_list.items():
    video_path = os.path.join(video_dir, video_name)

    text_content.append(description)
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)
    metadata = {"video": video_name, "fps": fps, "total_frames": total_frames}
    video_metadata_list.append(metadata)

    mod = int(fps // number_of_frames_per_second)
    if mod == 0:
        mod = 1
    frame_count = 0
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        frame_count += 1
        if frame_count % mod == 0:
            timestamp = (
                cap.get(cv2.CAP_PROP_POS_MSEC) / 1000
            )  # Convert milliseconds to seconds
            frame_path = os.path.join(frame_dir, f"{video_name}_{frame_count}.jpg")
            cv2.imwrite(frame_path, frame)  # Save the frame as an image
            frame_metadata = {
                "timestamp": timestamp,
                "frame_path": frame_path,
                "video": video_name,
                "frame_num": frame_count,
            }
            uris.append(frame_path)
            frame_metadata_list.append(frame_metadata)
    cap.release()

text_db.add_texts(text_content, video_metadata_list)
image_db.add_images(uris, frame_metadata_list);

"""
## Run Multimodal Retrieval

Here we define helper functions for retrieving text and image results based on a user query.
First, we use multi-modal retrieval to retrieve one text and three image documents for the user query. 
Then we return the video name for the video with the most results.
"""
logger.info("## Run Multimodal Retrieval")

def MultiModalRetrieval(
    query: str,
    n_texts: Optional[int] = 1,
    n_images: Optional[int] = 3,
    print_text_content=False,
):
    text_config = {"configurable": {"k_text_docs": {"k": n_texts}}}
    image_config = {"configurable": {"k_image_docs": {"k": n_images}}}

    logger.debug("\tRetrieving 1 text doc and 3 image docs")
    text_results = text_retriever.invoke(query, config=text_config)
    image_results = image_retriever.invoke(query, config=image_config)

    if print_text_content:
        logger.debug(
            f"\tPage content:\n\t\t{text_results[0].page_content}\n\n\tMetadata:\n\t\t{text_results[0].metadata}"
        )

    return text_results + image_results


def get_top_doc(results, qcnt=0):
    hit_score = {}
    for r in results:
        if "video" in r.metadata:
            video_name = r.metadata["video"]
            if video_name not in hit_score.keys():
                hit_score[video_name] = 0
            hit_score[video_name] += 1

    x = dict(sorted(hit_score.items(), key=lambda item: -item[1]))

    if qcnt >= len(x):
        return None
    return {"video": list(x)[qcnt]}


def Retrieve_top_results(prompt, qcnt=0, print_text_content=False):
    logger.debug("Querying database . . . ")
    results = MultiModalRetrieval(
        prompt, n_texts=1, n_images=3, print_text_content=print_text_content
    )
    logger.debug("Retrieved Top matching video!\n\n")

    top_doc = get_top_doc(results, qcnt)
    if top_doc is None:
        return None, None

    return top_doc["video"], top_doc

"""
Now let's query for a `man wearing khaki pants` and retrieve the top results.
"""
logger.info("Now let's query for a `man wearing khaki pants` and retrieve the top results.")

input_query = "Find a man wearing khaki pants"
video_name, top_doc = Retrieve_top_results(input_query, print_text_content=True)

"""
## Run RAG using LLM
### Load LLM Model
In this example, we use Meta's [LLama-2-Chat (7B) model](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) which is optimized for dialogue use cases.  
If you do not have access to this model, feel free to substitute the model with a different LLM.
In this example, the model is expected to be in `data/visual/llama-2-7b-chat-hf`.
"""
logger.info("## Run RAG using LLM")

model_path = str(datapath / "llama-2-7b-chat-hf")

model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float32,
    device_map="auto",
    trust_remote_code=True,
)

tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
tokenizer.padding_size = "right"
streamer = TextIteratorStreamer(tokenizer, skip_prompt=True)


class CustomLLM(LLM):
    @torch.inference_mode()
    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        streamer: Optional[TextIteratorStreamer] = None,  # Add streamer as an argument
    ) -> str:
        tokens = tokenizer.encode(prompt, return_tensors="pt")

        with torch.no_grad():
            output = model.generate(
                input_ids=tokens.to(model.device.type),
                max_new_tokens=100,
                num_return_sequences=1,
                num_beams=1,
                min_length=1,
                top_p=0.9,
                top_k=50,
                repetition_penalty=1.2,
                length_penalty=1,
                temperature=0.1,
                streamer=streamer,
                do_sample=True,
            )

    def stream_res(self, prompt):
        thread = Thread(
            target=self._call, args=(prompt, None, None, streamer)
        )  # Pass streamer to _call
        thread.start()

        for text in streamer:
            yield text

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        return model_path  # {"name_of_model": model_path}

    @property
    def _llm_type(self) -> str:
        return "custom"


llm = CustomLLM()

"""
### Run Chatbot

First, we define the prompt and a simple chatbot for processing the user query.
"""
logger.info("### Run Chatbot")

def get_formatted_prompt(scene, prompt):
    PROMPT = """ <<SYS>>
    You are an Intel assistant who understands visual and textual content.
    <</SYS>>
    [INST]
    You will be provided with two things, scene description and user's question. You are suppose to understand scene description \
    and provide answer to user's question.

    As an assistant, you need to follow these Rules while answering questions,

    Rules:
    - Don't answer any question which are not related to provided scene description.
    - Don't be toxic and don't include harmful information.
    - Answer if you can from provided scene description otherwise just say You don't have enough information to answer the question.

    Here is the,
    Scene Description: {{ scene }}

    The user wants to know,
    User: {{ prompt }}
    [/INST]\n
    Assistant:
    """
    return PROMPT.replace("{{ scene }}", scene).replace("{{ prompt }}", prompt)


def simple_chatbot(user_query):
    messages = [{"role": "assistant", "content": "How may I assist you today?"}]
    messages.append({"role": "user", "content": user_query})
    video_name, top_doc = Retrieve_top_results(user_query)

    scene_des = video_list[video_name]
    formatted_prompt = get_formatted_prompt(scene=scene_des, prompt=user_query)
    full_response = f"Most relevant retrieved video is **{video_name}** \n\n"
    for new_text in llm.stream_res(formatted_prompt):
        full_response += new_text
    message = {"role": "assistant", "content": full_response}
    messages.append(message)

    for message in messages:
        logger.debug(message["role"].capitalize(), ": ", message["content"])

    video_path = os.path.join(video_dir, top_doc["video"])
    return video_path

"""
Now let's use the simple chatbot to process a query asking for a `man holding a red shopping basket` and display the resulting video.
"""
logger.info("Now let's use the simple chatbot to process a query asking for a `man holding a red shopping basket` and display the resulting video.")

input_query = "Find a man holding a red shopping basket"
video_name, top_doc = Retrieve_top_results(input_query, print_text_content=True)

input_query = "Find a man holding a red shopping basket"
video_path = simple_chatbot(input_query)

Video(video_path, embed=True, width=640, height=360)

"""
## Stop VDMS Server
Now that we are done with the VDMS server, we can stop and remove it.
"""
logger.info("## Stop VDMS Server")

# ! docker kill vdms_rag_nb

"""
#
"""
logger.info("#")

logger.info("\n\n[DONE]", bright=True)