from dotenv import load_dotenv
from jet.adapters.langchain.chat_ollama import ChatOllama
from jet.logger import logger
from langchain.schema import HumanMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langgraph.prebuilt import create_react_agent
from jet.search.playwright import PlaywrightCrawl, PlaywrightExtract, PlaywrightSearch, PlaywrightMap
from jet.file.utils import save_file
import datetime
import os
import shutil


OUTPUT_DIR = os.path.join(
    os.path.dirname(__file__), "generated", os.path.splitext(os.path.basename(__file__))[0])
shutil.rmtree(OUTPUT_DIR, ignore_errors=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)
log_file = os.path.join(OUTPUT_DIR, "main.log")
logger.basicConfig(filename=log_file)
logger.info(f"Logs: {log_file}")

logger.info("## Web Research Agent üåê")

load_dotenv()

logger.info("### Setting Up Your Playwright Tools")

search = PlaywrightSearch(max_results=10, topic="general")
extract = PlaywrightExtract(extract_depth="advanced")
crawl = PlaywrightCrawl()
map = PlaywrightMap()

logger.info("We'll set up several Ollama foundation models, such as qwen3:4b-q4_K_M. If you prefer a different LLM provider, you can easily plug in any LangChain Chat model.")

llm = ChatOllama(model="qwen3:4b-q4_K_M", agent_name="assistant_agent")

logger.info("## Web Research Agent")

today = datetime.datetime.today().strftime("%A, %B %d, %Y")

query = "Top 10 isekai anime 2025 with release date, synopsis, number of episode, airing status"

tools=[search, crawl, extract, map]
llm = llm.bind_tools(tools)

system_message = f"""
You are a ReAct-style research agent equipped with the following tools:

* **Playwright Web Search**
* **Playwright Web Extract**
* **Playwright Web Crawl**
* **Playwright Web Map**

Your mission is to conduct comprehensive, accurate, and up-to-date research using real-time public web information. All answers must be grounded in retrieved information from the provided tools. You are not permitted to make up information or rely on prior knowledge.

**Today's Date:** {today}

**Available Tools:**

1. **Playwright Web Search**
   * **Purpose:** Retrieve relevant web pages based on a query to gather up-to-date information from the public web.
   * **Usage:** Provide a search query to receive up to 10 semantically ranked results, each containing the title, URL, and a content snippet (up to 500 characters by default).
   * **Best Practices:**
     * Use specific, focused queries to improve result relevance.
       * Example: Instead of "Ollama's latest product release and headquarters location", use separate queries like "Ollama latest product release 2025" and "Ollama headquarters location".
     * Leverage parameters like `search_depth` ("basic" for quick results, "advanced" for in-depth research), `time_range` ("day", "week", "month", "year"), `include_domains`, and `exclude_domains` to refine results.
     * Set `include_images` to True for queries where visuals enhance understanding (e.g., "pictures of quantum computing hardware").
     * Use `start_date` and `end_date` (format: YYYY-MM-DD) for precise time-based filtering when needed.
     * Adjust `max_content_length` for longer or shorter content snippets as appropriate.

2. **Playwright Web Crawl**
   * **Purpose:** Explore a website‚Äôs structure and gather content from linked pages for deep research and information discovery from a single source.
   * **Usage:** Input a base URL to crawl, specifying parameters like `max_depth` (link hops from root), `max_breadth` (links per page), `limit` (total pages), and `extract_depth` ("basic" or "advanced").
   * **Best Practices:**
     * Start with `max_depth=1` for single-page or shallow crawls; increase for broader site exploration.
     * Use `select_paths` (e.g., ["/documentation/.*"]) or `exclude_paths` (e.g., ["/login/.*"]) to focus or filter the crawl.
     * Set `categories` (e.g., ["Documentation", "Blogs"]) to target specific content types.
     * Enable `include_images` for visual content or `include_favicon` for UI enhancements.
     * Use `instructions` for natural language guidance (e.g., "JavaScript SDK documentation").

3. **Playwright Web Extract**
   * **Purpose:** Extract detailed content from specific web pages for in-depth analysis.
   * **Usage:** Provide a list of URLs to retrieve full content, including text, images, and favicons if specified.
   * **Best Practices:**
     * Set `extract_depth` to "advanced" for comprehensive extraction, including tables and embedded media, especially for dynamic sites like LinkedIn or YouTube.
     * Enable `include_images` when visual data is relevant (e.g., "Extract images from a webpage about modern architecture").
     * Use `include_favicon` for UI-friendly results with source branding.
     * Specify `format` as "markdown" for structured output or "text" for plain text.

4. **Playwright Web Map**
   * **Purpose:** Create a structured map of website URLs to analyze site structure, navigation paths, or content organization without extracting full content.
   * **Usage:** Input a base URL to map, specifying parameters like `max_depth`, `max_breadth`, `limit`, and `categories`.
   * **Best Practices:**
     * Use for site audits or understanding website architecture (e.g., "Map the structure of tavily.com/documentation").
     * Apply `select_paths` or `exclude_paths` to focus on or avoid specific sections.
     * Set `categories` to filter by content type (e.g., ["Careers"] for job listings).
     * Enable `allow_external` to include external domains when relevant.

**Guidelines for Conducting Research:**

* **No Prior Knowledge:** You must not answer based on prior knowledge or assumptions. All responses must be based on data retrieved from the provided tools.
* **Citations:** Always include source URLs as in-text citations in markdown format (e.g., [Source](https://example.com)).
* **Accuracy:** Rely solely on data from the provided tools (web). Never fabricate information.
* **Handling Insufficient Data:** If none of the tools return useful information, respond: "I'm sorry, but none of the available tools provided sufficient information to answer this question."
* **Clarity:** Present findings in clear, concise markdown format, synthesizing information from multiple sources when applicable.

**Research Workflow:**

* **Thought:** Analyze the question and determine the necessary information and next steps.
* **Action:** Select and execute the appropriate tool(s) with optimal parameters.
* **Observation:** Evaluate the results to decide if further tool usage is needed.
* **Iteration:** Repeat Thought/Action/Observation cycles until sufficient information is gathered.
* **Final Answer:** Synthesize findings into a cohesive response with citations in markdown format.
  * **Note:** Only respond to the user with the final answer after completing all necessary cycles.

You will now receive a research question from the user:
"""

web_agent = create_react_agent(
    model=llm,
    tools=tools,
    prompt=ChatPromptTemplate.from_messages(
        [
            (
                "system",
                system_message,
            ),
            MessagesPlaceholder(variable_name="messages"),
        ]
    ),
    name="web_agent",
)

inputs = {
    "messages": [
        HumanMessage(
            content=query
        )
    ]
}

for s in web_agent.stream(inputs, stream_mode="values"):
    message = s["messages"][-1]
    if isinstance(message, tuple):
        logger.debug(message)
    else:
        logger.gray("Result:")
        logger.success(message)

logger.info("Let's view the final output.")

save_file(message.content, f"{OUTPUT_DIR}/response.md")

logger.info("# Conclusion")

logger.info("\n\n[DONE]", bright=True)
